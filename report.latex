\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, positioning, fit, automata}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\title{L1 Cache Simulator for Quad-Core Processors with MESI Protocol}
\author{Vanshika(2023CS10746) Shivankur Gupta(2023CS10809)\\
Department of Computer Science and Engineering\\
Indian Institute of Technology Delhi
}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Introduction}
This report presents the design and implementation of an L1 cache simulator for quad-core processors with cache coherence support. The simulator models a system where each of the four processor cores has its own L1 data cache, which are kept coherent using the MESI (Modified, Exclusive, Shared, Invalid) cache coherence protocol. The caches are connected via a central snooping bus that facilitates coherence transactions.

Cache coherence is a critical aspect of multiprocessor systems to ensure that all cores have a consistent view of memory. The MESI protocol is one of the most commonly used coherence protocols in modern processors, which maintains consistency by tracking the state of each cache line and appropriately handling read and write operations across multiple caches.

\subsection{Problem Statement}
The objective is to simulate L1 caches in a quad-core processor system with MESI cache coherence protocol support. The simulation is trace-driven, where memory reference traces from four processor cores are provided as input. The simulator tracks various statistics including hit/miss rates, writebacks, invalidations, and bus traffic, which help analyze the cache performance and coherence protocol behavior.

\subsection{MESI Protocol Overview}
The MESI protocol defines four states for each cache line:
\begin{itemize}
    \item \textbf{Modified (M)}: The cache line is present only in the current cache and is dirty (has been modified). The main memory copy is stale.
    \item \textbf{Exclusive (E)}: The cache line is present only in the current cache and is clean (matches main memory).
    \item \textbf{Shared (S)}: The cache line may be present in other caches and is clean.
    \item \textbf{Invalid (I)}: The cache line is invalid and must be fetched from memory or another cache if needed.
\end{itemize}

State transitions occur based on processor requests and bus snooping events, ensuring data consistency across all caches.

\section{System Architecture}

\subsection{Overall Architecture}
The simulator models a quad-core multiprocessor system with a hierarchical memory architecture designed to balance performance, coherence, and complexity. The system consists of the following primary components:

\begin{itemize}
    \item \textbf{Four Processor Cores}: Independent computational units that execute instructions from their respective instruction streams. Each core generates memory access requests (reads and writes) to its private L1 cache based on the trace input. The cores operate in parallel but are synchronized at the cycle level through the shared bus.
    
    \item \textbf{Private L1 Data Caches}: Each processor core is equipped with its own private L1 data cache. These caches serve as the first level of the memory hierarchy and are designed to capture temporal and spatial locality in memory access patterns. Each cache is organized as a set-associative structure with configurable parameters (set index bits, associativity, and block size). The caches implement the MESI coherence protocol to maintain data consistency across the system.
    
    \item \textbf{Central Snooping Bus}: A shared communication medium that connects all four L1 caches and the main memory. The bus serves two critical functions: (1) it provides a path for data transfer between caches and memory, and (2) it enables cache coherence by broadcasting memory access requests to all caches, allowing them to "snoop" on each other's transactions. The bus follows a split-transaction protocol where a request and its corresponding response can be separated in time.
    
    \item \textbf{Main Memory}: The backing store that holds the complete address space accessible to all processors. It represents the slowest but largest component of the memory hierarchy. The main memory is accessed when data cannot be found in or shared among the L1 caches, or when modified data needs to be written back from caches.
    
    \item \textbf{Memory Address Space}: A unified 32-bit physical address space that is shared by all four cores. Each address uniquely identifies a byte in main memory. For cache addressing, this 32-bit address is decomposed into tag bits, set index bits, and block offset bits based on the cache configuration parameters.
\end{itemize}

The memory hierarchy is organized to exploit locality principles while maintaining coherence:
\begin{itemize}
    \item \textbf{Data Flow}: Memory access requests originate from processor cores and are first directed to the respective L1 caches. Upon a cache hit, data is served directly from the cache. Upon a miss, the request is broadcast on the bus to check if other caches have the data and to access main memory if necessary.
    
    \item \textbf{Coherence Flow}: Write operations may invalidate or update copies of the same data in other caches. The MESI protocol manages the state transitions to ensure that all cores have a consistent view of memory despite potential data sharing and parallel modifications.
\end{itemize}

\begin{figure}[h]
\centering
\begin{tikzpicture}[node distance=1.5cm, auto]
    % Define styles
    \tikzstyle{core} = [rectangle, rounded corners, minimum width=2cm, minimum height=1cm, text centered, draw=black, fill=blue!10]
    \tikzstyle{cache} = [rectangle, rounded corners, minimum width=2cm, minimum height=1cm, text centered, draw=black, fill=green!10]
    \tikzstyle{memory} = [rectangle, rounded corners, minimum width=7cm, minimum height=1cm, text centered, draw=black, fill=yellow!10]
    \tikzstyle{bus} = [rectangle, rounded corners, minimum width=7cm, minimum height=0.7cm, text centered, draw=black, fill=red!10]
    \tikzstyle{arrow} = [thick,->,>=stealth]
    
    % Place nodes
    \node (core0) [core] {Core 0};
    \node (cache0) [cache, below of=core0] {L1 Cache 0};
    \node (core1) [core, right=2cm of core0] {Core 1};
    \node (cache1) [cache, below of=core1] {L1 Cache 1};
    \node (core2) [core, right=2cm of core1] {Core 2};
    \node (cache2) [cache, below of=core2] {L1 Cache 2};
    \node (core3) [core, right=2cm of core2] {Core 3};
    \node (cache3) [cache, below of=core3] {L1 Cache 3};
    
    \node (bus) [bus, below=1.5cm of cache1] {Central Snooping Bus};
    \node (memory) [memory, below=1.2cm of bus] {Main Memory};
    
    % Draw edges
    \draw [arrow] (core0) -- (cache0);
    \draw [arrow] (core1) -- (cache1);
    \draw [arrow] (core2) -- (cache2);
    \draw [arrow] (core3) -- (cache3);
    
    \draw [arrow] (cache0) -- (bus);
    \draw [arrow] (cache1) -- (bus);
    \draw [arrow] (cache2) -- (bus);
    \draw [arrow] (cache3) -- (bus);
    
    \draw [arrow] (bus) -- (memory);
    \draw [arrow] (memory) -- (bus);
\end{tikzpicture}
\caption{Overall system architecture showing the four processor cores, their private L1 caches, the central snooping bus, and the shared main memory. Arrows indicate data and coherence message flows between components.}
\label{fig:architecture}
\end{figure}

\subsection{Assumptions}
The simulator is built upon a set of carefully considered assumptions that define the behavior and limitations of the modeled system. These assumptions clarify the scope of the simulation and ensure consistent interpretation of results:

\subsubsection{General assumptions}
\begin{itemize}
    \item Total Instructions : Corresponds to the length of the trace for the core.
    \item Total Reads : Total number of instructions which have R at the starting.
    \item Total Writes : Total number of instructions which have W at the starting.
    \item Total Execution Cycles : The cycles where the core is actively processing it's own request. i.e. the bus may be busy doing that core's request work or the core may have some cache hit. 
    \item Idle cycles : Are the cycles where the core's request is denied by the bus, due to the bus being busy. That is those cycles where the core is trying to issue the request to bus but is unable to do so.
    \item Cache Misses : These are the number of instructions which were a miss by the cache i.e. the line was not found in the cache.
    \item Cache Miss Rate : This is the number of misses per total number of instructions in the trace for that core. 
    \item Cache Evictions : The number of blocks removed from the cache and written back to the memory in the whole process till the simulation ends.
    \item Writebacks : The number of modified blocks removed from the cache and written back to the memory in the whole process till the simulation ends.
    \item Bus Invalidations : Whenever the core sends signal to other cores which have that block, to invalidate themselves. This includes both Write miss and Write Hit cases.
    \item Data Traffic : The total amount of data moved through the bus. This includes : 
        \begin{itemize}
            \item Any data transaction from memory to the cache or form the cache to memory. 
            \item Any data transaction which is form the cache to some other cache or from some other cache to the cache.
            
        \end{itemize}
    \item Bus Transactions : This is the total number of Read Misses, Write Misses and Write Hits when someone has  block in shared.
    \item Total Bus traffic : All the data transfer involving cache, memory etc. through the bus. 
\end{itemize}

\subsubsection{Bus Architecture and Behavior}
\begin{itemize}
    \item \textbf{Blocking Bus}: The central snooping bus is a blocking resource that can process only one transaction at a time. When a cache initiates a bus transaction, other caches must wait until the transaction completes before they can use the bus. This assumption reflects the physical reality of shared bus architectures where simultaneous transmission would lead to signal conflicts.
    
    \item \textbf{Cycle-by-Cycle Arbitration}: Bus access is arbitrated on a cycle-by-cycle basis. When multiple cores request bus access in the same cycle, the arbitration logic grants access based on a fixed priority scheme rather than implementing more complex policies like round-robin or age-based arbitration.
    
    \item \textbf{Bus Width and Bandwidth}: The bus width is assumed to match the cache block size, allowing an entire cache block to be transferred in a single bus transaction (though requiring multiple cycles). This simplifies the modeling of data transfers while maintaining realistic timing constraints.
    
    \item \textbf{Split-Transaction Protocol}: The bus supports a split-transaction protocol where request and response phases can be separated in time. This allows the bus to be released between the request and data transfer phases, improving bus utilization.

\end{itemize}

\subsubsection{Cache Coherence Protocol Implementation}
\begin{itemize}
    \item \textbf{Cache-to-Cache Transfers}: Direct cache-to-cache transfers are supported for shared data. When a core experiences a read miss and another cache has the requested data in a modified state, the data is transferred directly from the source cache to the requesting cache rather than first being written back to main memory. This optimization reduces memory traffic and latency.
    
    \item \textbf{Priority Based on Core ID}: When multiple coherence operations conflict, priority is given to the core with the lower ID. This deterministic policy ensures freedom from livelock and deadlock in the coherence protocol, though it may introduce fairness concerns in certain workloads.
    
    \item \textbf{Implicit Invalidation Acknowledgments}: Invalidation acknowledgments are modeled implicitly and do not consume additional bus cycles. When a core broadcasts an invalidation request, it is assumed that all other caches process and acknowledge the invalidation within the same cycle, without requiring explicit response messages. This simplification reduces protocol complexity while maintaining correctness.
    
    \item \textbf{Atomic Coherence Operations}: Cache coherence operations are treated as atomic from the perspective of the processors. Once a coherence operation begins, the affected cache line is locked until the operation completes, preventing race conditions in the protocol implementation.
\end{itemize}

\subsubsection{Memory System Behavior}
\begin{itemize}
    \item \textbf{Fixed Memory Access Latency}: Main memory access has a fixed latency of 100 cycles, regardless of access patterns or potential optimizations like DRAM row buffering. This simplification allows for consistent performance modeling while still capturing the significant latency gap between cache and memory accesses.
    
    \item \textbf{Write-Back Policy}: All caches implement a write-back policy where modified data is only written to the next level (in this case, main memory) when the cache line is evicted or explicitly required by the coherence protocol. This reduces bus traffic compared to a write-through policy.
    
    \item \textbf{Write-Allocate Policy}: Upon a write miss, the system allocates a cache line and fetches the data before performing the write (write-allocate), rather than writing directly to memory (no-write-allocate). This policy favors workloads with write locality at the cost of potentially unnecessary fetches.
    
    \item \textbf{Inclusive Memory Hierarchy}: The memory hierarchy is assumed to be inclusive, meaning that all data present in any L1 cache must also be present in main memory (though potentially with stale values until writeback occurs).
\end{itemize}

\subsubsection{Performance and Timing Model}
\begin{itemize}
    \item \textbf{Single-Cycle Cache Hit}: A cache hit is serviced in a single processor cycle, assuming an optimized cache design that can deliver data within the core's clock cycle.
    
    \item \textbf{Fixed Cache-to-Cache Transfer Latency}: Cache-to-cache transfers incur a latency of 2 cycles per word transferred, reflecting the overhead of inter-cache communication through the shared bus.
    
    \item \textbf{No Pipelining of Memory Accesses}: Memory accesses are not pipelined; each access must complete before the next one can begin. This simplifies the timing model at the cost of potentially underestimating performance in systems with overlapped memory operations.
    
    \item \textbf{Perfect Instruction Execution}: The simulator focuses on data cache performance and assumes perfect instruction fetch and execution (no instruction cache misses or pipeline stalls unrelated to data access).
\end{itemize}

These assumptions collectively define a realistic yet tractable model of a quad-core processor with private L1 caches and MESI coherence protocol. The model captures the essential behaviors and performance characteristics of real systems while making reasonable simplifications to keep the simulation manageable.

\section{Implementation Details}

\subsection{Class Structure}
The simulator is implemented using an object-oriented approach in C++, with the following main classes:

\begin{figure}[h]
\centering
\begin{tikzpicture}[node distance=2cm, auto]
    \tikzstyle{class} = [
        rectangle, 
        rounded corners, 
        minimum width=5cm, 
        minimum height=1.5cm, 
        text centered, 
        align=center, % <-- add this line
        draw=black, 
        fill=blue!5
    ]
    \tikzstyle{arrow} = [thick,->,>=stealth]
    
    \node (core) [class] {Core\\\small{Manages trace processing and statistics}};
    \node (cache) [class, right=3cm of core] {Cache\\\small{Implements cache operations and MESI}};
    \node (bus) [class, below=2cm of cache] {Bus\\\small{Manages coherence transactions}};
    \node (cacheset) [class, below=2cm of core] {CacheSet\\\small{Manages a set of cache lines}};
    \node (cacheline) [class, right=3cm of cacheset] {CacheLine\\\small{Represents a single cache line}};
    
    \draw [arrow] (core) -- (cache) node[midway, above] {has-a};
    \draw [arrow] (cache) -- (bus) node[midway, right] {uses};
    \draw [arrow] (cache) -- (cacheset) node[midway, left] {contains};
    \draw [arrow] (cacheset) -- (cacheline) node[midway, above] {contains};
\end{tikzpicture}
\caption{Class diagram showing relationships between Core, Cache, and associated components}
\end{figure}


\begin{table}[h]
\centering
\begin{tabular}{|c|p{10cm}|}
\hline
\textbf{Component} & \textbf{Description} \\
\hline
Core & Manages trace processing and statistics. \\
\hline
Cache & Implements cache operations and MESI protocol. \\
\hline
Bus & Manages coherence transactions. \\
\hline
CacheSet & Manages a set of cache lines. \\
\hline
CacheLine & Represents a single cache line. \\
\hline
\end{tabular}
\caption{Class descriptions}
\label{tab:class_descriptions}
\end{table}

\subsection{Data Structures}
\subsubsection{CacheLine}
Represents a single cache line with the following attributes:
\begin{itemize}
    \item \texttt{tag}: 32-bit tag value
    \item \texttt{state}: MESI state (Modified, Exclusive, Shared, Invalid, or Empty)
    \item \texttt{lastUsedCycle}: Timestamp for LRU replacement
\end{itemize}

\subsubsection{CacheSet}
Represents a set of cache lines with the following attributes:
\begin{itemize}
    \item \texttt{lines}: Vector of CacheLine objects
    \item Methods for finding lines by tag, identifying victims, and updating LRU information
\end{itemize}

\subsubsection{Cache}
Represents the L1 cache of a processor core:
\begin{itemize}
    \item \texttt{sets}: Vector of CacheSet objects
    \item Configuration parameters: set index bits (s), associativity (E), block bits (b)
    \item Methods for handling memory accesses and snooping requests
\end{itemize}

\subsubsection{Bus}
Represents the central snooping bus:
\begin{itemize}
    \item \texttt{caches}: Vector of pointers to Cache objects
    \item Statistics counters: invalidations, data traffic, transactions
    \item Methods for broadcasting coherence messages
\end{itemize}

\subsubsection{Core}
Represents a processor core:
\begin{itemize}
    \item \texttt{cache}: Pointer to associated Cache object
    \item Statistics counters: accesses, hits, misses, writebacks, etc.
    \item Methods for processing memory access traces
\end{itemize}

\section{Implementation Flow}

\subsection{Cache Access Flow}
The following algorithm describes the cache access operation:

\begin{algorithm}[H]
\caption{Cache Access Procedure}
\begin{algorithmic}[1]
\Function{Cache::access}{address, operation, cycle, penaltyCycles}
    \State Extract tag, set index, and block offset from address
    \State Find the corresponding set
    \State Search for the tag in the set
    \If{line is found AND state is not INVALID}
        \State Update LRU for the accessed line
        \If{operation is WRITE}
            \If{state is SHARED}
                \State Check if bus is busy
                \If{bus is busy}
                    \State \Return \{true, true\} \Comment{Retry later}
                \EndIf
                \State Broadcast invalidate on bus
                \State Add bus delay
            \EndIf
            \State Set line state to MODIFIED
        \EndIf
        \State \Return \{true, false\} \Comment{Cache hit}
    \EndIf
    
    \State Check if bus is busy
    \If{bus is busy}
        \State \Return \{false, true\} \Comment{Retry later}
    \EndIf
    
    \State Find victim line for replacement
    \If{victim line is MODIFIED}
        \State Writeback to memory (add penalty)
        \State Broadcast writeback on bus
        \State Update writeback statistics
    \EndIf
    
    \If{operation is READ}
        \State Broadcast BusRd on bus
        \State Set line state based on response (SHARED or EXCLUSIVE)
        \State Add appropriate memory access penalty
    \Else
        \State Broadcast BusRdX on bus
        \State Set line state to MODIFIED
        \State Add memory access penalty
    \EndIf
    
    \State \Return \{false, false\} \Comment{Cache miss}
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Cache Snooping Flow}
The following algorithm describes the cache snooping operation:

\begin{algorithm}[H]
\caption{Cache Snooping Procedure}
\begin{algorithmic}[1]
\Function{Cache::snoop}{address, operation, penaltyCycles}
    \State Extract tag and set index from address
    \State Find the corresponding set
    \State Search for the tag in the set
    \If{line not found OR state is INVALID}
        \State \Return false \Comment{No response needed}
    \EndIf
    
    \If{operation is READ (BusRd)}
        \If{state is MODIFIED}
            \State Writeback to memory (add penalty)
            \State Broadcast writeback on bus
            \State Update writeback statistics
        \EndIf
        \State Change state to SHARED
        \State \Return true
    \ElsIf{operation is WRITE (BusRdX or Invalidate)}
        \If{state is MODIFIED}
            \State Writeback to memory (add penalty)
            \State Broadcast writeback on bus
            \State Update writeback statistics
        \EndIf
        \State Change state to INVALID
        \State \Return true
    \EndIf
    
    \State \Return false
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{MESI State Transitions}
The MESI protocol state transitions are implemented as shown in the following state diagrams:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{mesi_local.png}
    \caption{MESI protocol state transitions for locally initiated accesses. The diagram shows read/write hit/miss scenarios and the resulting state transitions. Green boxes represent bus transactions.}
    \label{fig:mesi_local}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{mesi_remote.png}
    \caption{MESI protocol state transitions for remotely initiated accesses. Arrows indicate state transitions, with green boxes showing the operations that trigger them. Black circles represent copy-back operations.}
    \label{fig:mesi_remote}
\end{figure}

\subsection{Simulation Flow}
The main simulation loop processes memory references from each core in a cycle-by-cycle manner:

\begin{algorithm}[H]
\caption{Main Simulation Loop}
\begin{algorithmic}[1]
\State Initialize bus and caches
\State Load trace files for each core
\State currentCycle = 0
\State finished = false
\While{not finished}
    \State finished = true
    \For{each core}
        \If{core has more instructions OR waiting to repeat}
            \State finished = false
            \State Process next memory reference
        \EndIf
    \EndFor
    \State currentCycle++
\EndWhile
\State Output statistics
\end{algorithmic}
\end{algorithm}

\section{Coherence Operations}

\subsection{Bus Transactions}
The simulator implements the following bus transactions:

\begin{itemize}
    \item \textbf{BusRd (R)}: Read request, issued when a processor has a read miss
    \item \textbf{BusRdX (W)}: Read with intent to modify, issued when a processor has a write miss
    \item \textbf{Invalidate (I)}: Invalidate request, issued when a processor writes to a shared line
    \item \textbf{Writeback (B)}: Writeback notification, issued when a dirty line is evicted
\end{itemize}

\subsection{Cache-to-Cache Transfers}
When a processor issues a BusRd and another processor has the requested data in Modified state, the data is transferred directly from the cache with the data to the requesting cache. This is more efficient than fetching from memory and helps reduce memory traffic.

\subsection{Handling Write Hits to Shared Lines}
When a processor wants to write to a line in Shared state, it first broadcasts an Invalidate message on the bus to force other caches to invalidate their copies, ensuring exclusive access before modification.

\section{Performance Analysis}

\subsection{Timing Model}
The simulator uses the following timing model:
\begin{itemize}
    \item Cache hit: 1 cycle
    \item Memory access: 100 cycles
    \item Cache-to-cache transfer: 2 cycles per word
    \item Writeback to memory: 100 cycles
\end{itemize}

The total execution time for each core is the sum of:
\begin{itemize}
    \item Hit time (1 cycle per hit)
    \item Miss penalty (additional cycles for misses)
    \item Bus contention delay (when the bus is busy)
\end{itemize}

\subsection{Statistics Collection}
The simulator collects the following statistics for analysis:
\begin{itemize}
    \item Number of reads and writes per core
    \item Total execution cycles per core
    \item Idle cycles per core (waiting for memory or bus)
    \item Cache miss rate per core
    \item Number of evictions per core
    \item Number of writebacks per core
    \item Number of invalidations on the bus
    \item Total data traffic (in bytes) on the bus
\end{itemize}

\section{Results and Analysis}

\subsection{Experimental Setup}
We evaluated the simulator using trace files from parallel applications running on a quad-core processor. Multiple experiments were conducted by systematically varying cache parameters—specifically cache size (through varying sets), associativity, and block size—to study their impact on system performance. The experiments used the same input traces across all configurations to ensure consistent workload characteristics.

For each configuration, the following metrics were measured:
\begin{itemize}
    \item Maximum execution time across all cores (in processor cycles)
    \item Per-core execution times
    \item Cache miss rates
    \item Bus traffic and coherence messages
\end{itemize}

\subsection{Effect of Cache Size}
Increasing the cache size generally improves performance by reducing the miss rate and consequently the execution time. Figure \ref{fig:cache_size_impact} shows the relationship between cache size and execution time.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{cache_size_impact.png}
    \caption{Impact of Cache Size on Maximum Execution Time}
    \label{fig:cache_size_impact}
\end{figure}

Our experimental results, summarized in Table \ref{tab:cache_size}, demonstrate that doubling the cache size from 2KB to 4KB yields a dramatic 45.5\% reduction in execution time (from 13.17M to 7.18M cycles). Further doubling the cache size from 4KB to 8KB provides an additional 21.9\% improvement. This indicates diminishing returns as cache size increases, which is typical in computer systems where the working set of the application starts to fit within the cache.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Cache Size (KB)} & \textbf{Sets} & \textbf{Associativity} & \textbf{Block Size (bytes)} & \textbf{Execution Time (cycles)} \\
\hline
2.0 & 32 & 2 & 32 & 13,166,569 \\
\hline
4.0 & 64 & 2 & 32 & 7,179,569 \\
\hline
8.0 & 128 & 2 & 32 & 5,607,101 \\
\hline
\end{tabular}
\caption{Performance metrics for different cache sizes (with fixed associativity E=2 and block size b=5)}
\label{tab:cache_size}
\end{table}

The performance improvement with larger caches is attributed to:
\begin{itemize}
    \item Reduced capacity misses as more of the working set fits in the cache
    \item Lower eviction rates, resulting in fewer writebacks and less bus traffic
    \item Decreased coherence traffic as data remains in caches longer
\end{itemize}

\subsection{Effect of Associativity}
Associativity has a profound impact on cache performance, especially in multi-core systems where conflict misses can be prevalent. Figure \ref{fig:associativity_impact} illustrates how execution time varies with different associativity values.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{associativity_impact.png}
    \caption{Impact of Associativity on Maximum Execution Time}
    \label{fig:associativity_impact}
\end{figure}

Table \ref{tab:associativity} presents our experimental findings, which show that increasing associativity from direct-mapped (E=1) to 2-way set-associative (E=2) results in a dramatic 84.6\% reduction in execution time. However, further increasing associativity to 4-way (E=4) yields only a modest 1.4\% additional improvement.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Associativity} & \textbf{Sets} & \textbf{Block Size (bytes)} & \textbf{Cache Size (KB)} & \textbf{Execution Time (cycles)} \\
\hline
1-way & 64 & 32 & 2.0 & 46,656,317 \\
\hline
2-way & 64 & 32 & 4.0 & 7,179,569 \\
\hline
4-way & 64 & 32 & 8.0 & 5,531,229 \\
\hline
\end{tabular}
\caption{Performance metrics for different associativities (with fixed sets s=6)}
\label{tab:associativity}
\end{table}

This dramatic improvement when moving from direct-mapped to set-associative caches can be explained by:
\begin{itemize}
    \item Significant reduction in conflict misses, which are particularly problematic in direct-mapped caches
    \item Better handling of program access patterns that may conflict in the same set
    \item Improved effectiveness of the LRU replacement policy, which can only be applied when multiple lines exist within a set
    \item Better resilience to pathological access patterns that can cause thrashing in direct-mapped caches
\end{itemize}

The diminishing returns observed when increasing associativity beyond 2-way suggest that for this workload, most conflict misses are already eliminated with 2-way associativity, and higher associativity primarily benefits corner cases.

\subsection{Effect of Cache Sets (Set Index Bits)}
The number of sets in a cache affects its organization and can impact performance independently of total cache size. Figure \ref{fig:sets_impact} shows how execution time changes as the number of set index bits (s) increases.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{sets_impact.png}
    \caption{Impact of Cache Sets (Set Index Bits) on Maximum Execution Time}
    \label{fig:sets_impact}
\end{figure}

With fixed associativity and block size, increasing the number of sets directly increases the cache size. Our results show a steady decrease in execution time as s increases from 5 to 7, corresponding to an increase from 32 to 128 sets. This is consistent with our previous observations on cache size effects.

\subsection{Effect of Block Size}
Block size impacts both spatial locality exploitation and bus traffic. Figure \ref{fig:block_size_impact} illustrates the relationship between block size and execution time.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{block_size_impact.png}
    \caption{Impact of Block Size on Maximum Execution Time}
    \label{fig:block_size_impact}
\end{figure}

As shown in Table \ref{tab:block_size}, increasing the block size from 16 bytes to 32 bytes results in a 52.2\% reduction in execution time. Further increasing to 64 bytes provides a more modest 13.3\% additional improvement.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Block Size (bytes)} & \textbf{Sets} & \textbf{Associativity} & \textbf{Cache Size (KB)} & \textbf{Execution Time (cycles)} \\
\hline
16 & 64 & 2 & 2.0 & 15,008,389 \\
\hline
32 & 64 & 2 & 4.0 & 7,179,569 \\
\hline
64 & 64 & 2 & 8.0 & 6,227,913 \\
\hline
\end{tabular}
\caption{Performance metrics for different block sizes (with fixed sets s=6 and associativity E=2)}
\label{tab:block_size}
\end{table}

The significant performance improvement with larger block sizes can be attributed to:
\begin{itemize}
    \item Better exploitation of spatial locality, as programs often access memory locations that are near each other
    \item Amortization of miss penalty over larger data chunks, reducing the average miss cost
    \item More efficient bus utilization for block transfers
\end{itemize}

However, the diminishing returns observed with very large block sizes (64 bytes) suggest a tradeoff:
\begin{itemize}
    \item Larger blocks may bring in data that is never used, wasting bandwidth
    \item Larger blocks take longer to transfer on the bus, potentially increasing bus contention
    \item The increased miss penalty for larger blocks can offset the reduced miss rate
\end{itemize}

\subsection{Combined Parameter Effects}
Figure \ref{fig:combined_parameters} illustrates how different cache parameters interact to affect execution time. When examining multiple parameter variations simultaneously, we observe that:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{combined_parameters.png}
    \caption{Impact of Cache Size on Maximum Execution Time (All Parameters)}
    \label{fig:combined_parameters}
\end{figure}

\begin{itemize}
    \item Associativity (E) has the most dramatic impact on performance, especially when moving from direct-mapped (E=1) to set-associative caches
    \item Block size (b) and the number of sets (s) both show significant but diminishing returns as they increase
    \item At larger cache sizes (8KB), the performance differences between parameter variations become less pronounced, indicating that most critical misses are being eliminated regardless of the specific organization
\end{itemize}

\subsection{Core-to-Core Variations}
Our results also reveal interesting variations in execution times across different cores. In all configurations, Core 2 consistently experiences the longest execution time. This suggests that the workload distribution or memory access patterns differ significantly across cores, with Core 2 likely experiencing more cache misses, coherence invalidations, or bus contention than the other cores.

For example, in the baseline configuration (s=6, E=2, b=5), the execution times are:
\begin{itemize}
    \item Core 0: 6,382,305 cycles
    \item Core 1: 6,269,780 cycles
    \item Core 2: 7,179,569 cycles (14.5\% longer than average)
    \item Core 3: 6,377,307 cycles
\end{itemize}

This variation highlights the importance of considering per-core performance in multi-core systems, as aggregate metrics may hide important load imbalances.

\subsection{Coherence Traffic Analysis}
The MESI protocol generates different types of coherence traffic that contribute to bus utilization and affect overall system performance. While detailed transaction counts were not provided in the results, the patterns observed in execution time correlate with expected coherence behavior:

\begin{itemize}
    \item Smaller caches generate more coherence traffic due to increased evictions and subsequent re-fetches
    \item Lower associativity (especially direct-mapped caches) results in higher conflict misses, leading to more invalidations and writebacks
    \item Larger block sizes reduce the number of transactions but increase the data volume per transaction
\end{itemize}

The dramatic performance improvement seen when moving from direct-mapped to set-associative caches likely corresponds to a significant reduction in coherence traffic, as fewer conflict misses means fewer invalidations and writebacks.

\subsection{Performance Optimality}
Based on our experimental results, the optimal cache configuration for the tested workload appears to be:
\begin{itemize}
    \item Sets: 128 (s=7)
    \item Associativity: 4-way (E=4)
    \item Block size: 32 bytes (b=5)
\end{itemize}

This configuration balances the benefits of larger cache size, sufficient associativity to avoid most conflict misses, and moderate block size that exploits spatial locality without excessive bus traffic. Further increases in these parameters would likely yield diminishing returns relative to the hardware cost.

\subsection{Core-to-Core Variations}
Our results also reveal interesting variations in execution times across different cores. In all configurations, Core 2 consistently experiences the longest execution time. This suggests that the workload distribution or memory access patterns differ significantly across cores, with Core 2 likely experiencing more cache misses, coherence invalidations, or bus contention than the other cores.

For example, in the baseline configuration (s=6, E=2, b=5), the execution times are:
\begin{itemize}
    \item Core 0: 6,382,305 cycles
    \item Core 1: 6,269,780 cycles
    \item Core 2: 7,179,569 cycles (14.5\% longer than average)
    \item Core 3: 6,377,307 cycles
\end{itemize}

This variation highlights the importance of considering per-core performance in multi-core systems, as aggregate metrics may hide important load imbalances.

\subsection{Coherence Traffic Analysis}
The MESI protocol generates different types of coherence traffic that contribute to bus utilization and affect overall system performance. While detailed transaction counts were not provided in the results, the patterns observed in execution time correlate with expected coherence behavior:

\begin{itemize}
    \item Smaller caches generate more coherence traffic due to increased evictions and subsequent re-fetches
    \item Lower associativity (especially direct-mapped caches) results in higher conflict misses, leading to more invalidations and writebacks
    \item Larger block sizes reduce the number of transactions but increase the data volume per transaction
\end{itemize}

The dramatic performance improvement seen when moving from direct-mapped to set-associative caches likely corresponds to a significant reduction in coherence traffic, as fewer conflict misses means fewer invalidations and writebacks.

\subsection{Performance Optimality}
Based on our experimental results, the optimal cache configuration for the tested workload appears to be:
\begin{itemize}
    \item Sets: 128 (s=7)
    \item Associativity: 4-way (E=4)
    \item Block size: 32 bytes (b=5)
\end{itemize}

This configuration balances the benefits of larger cache size, sufficient associativity to avoid most conflict misses, and moderate block size that exploits spatial locality without excessive bus traffic. Further increases in these parameters would likely yield diminishing returns relative to the hardware cost.

\section{Discussion}

\subsection{Cache Coherence Overhead}
The cache coherence protocol introduces overhead in terms of additional bus traffic and invalidations. However, it is necessary to maintain data consistency in multiprocessor systems. The MESI protocol efficiently reduces this overhead by distinguishing between exclusive and shared clean states, which allows write operations to exclusive cache lines without generating bus traffic.

\subsection{Performance Optimizations}
Several optimizations could improve the performance of the simulated system:

\begin{itemize}
    \item \textbf{Directory-based coherence}: For larger systems, a directory-based protocol might be more scalable than snooping.
    \item \textbf{Non-blocking caches}: Allowing the cache to handle multiple outstanding misses could improve performance.
    \item \textbf{Speculative execution}: Allowing the processor to continue execution past cache misses could hide memory latency.
    \item \textbf{Prefetching}: Fetching data before it is requested could reduce miss rates.
\end{itemize}

\subsection{Limitations of the Simulator}
The current simulator has some limitations:

\begin{itemize}
    \item No modeling of instruction cache effects
    \item No modeling of out-of-order execution or other advanced processor features
    \item Simplified bus arbitration model
    \item No consideration of DRAM timing parameters
\end{itemize}

\section{Conclusion}
We have implemented a detailed simulator for L1 caches in a quad-core processor system with MESI cache coherence protocol. The simulator accurately models cache operations, coherence transactions, and collects various performance metrics. The results highlight the importance of proper cache configuration and the impact of coherence protocol on system performance.

The simulator can be used to study the behavior of different applications on multi-core systems and to explore the design space of cache parameters for optimal performance. Future work could extend the simulator to include more advanced features such as non-blocking caches, prefetching, and directory-based coherence protocols.

\section{References}
\begin{enumerate}
    \item Hennessy, J. L., \& Patterson, D. A. (2011). Computer architecture: a quantitative approach. Elsevier.
    \item Sorin, D. J., Hill, M. D., \& Wood, D. A. (2011). A primer on memory consistency and cache coherence. Synthesis Lectures on Computer Architecture, 6(3), 1-212.
    \item Culler, D. E., Singh, J. P., \& Gupta, A. (1999). Parallel computer architecture: a hardware/software approach. Gulf Professional Publishing.
\end{enumerate}

\section{Code Listing}
The simulator is implemented using the following main classes:

\subsection{CacheLine.hpp}
\begin{lstlisting}[language=C++]
#pragma once
#include <cstdint>
#include <vector>

enum MESIState { MODIFIED, EXCLUSIVE, SHARED, INVALID, EMPTY };

struct CacheLine {
    uint32_t tag;
    MESIState state;
    int lastUsedCycle;
};
\end{lstlisting}

\subsection{CacheSet.hpp}
\begin{lstlisting}[language=C++]
#pragma once
#include "CacheLine.hpp"
#include <vector>

class CacheSet {
public:
    std::vector<CacheLine> lines;

    CacheSet(int associativity);
    int findLine(uint32_t tag);
    int findVictim();
    void updateLRU(int lineIndex, int currentCycle);
};
\end{lstlisting}

\subsection{Cache.hpp}
\begin{lstlisting}[language=C++]
#pragma once
#include "CacheSet.hpp"
#include <vector>

class Bus;
class Core;

class Cache {
private:
    int s, E, b;
    int numSets;
    std::vector<CacheSet> sets;
    Bus *bus; // Pointer to Bus
    int coreId;
    
public:
    Core* core; // Pointer to Core
    Cache(int s, int E, int b, int coreId, Bus *bus);
    void add_core(Core* core);
    std::pair<bool, bool> access(uint32_t address, char op, int cycle, int &penaltyCycles);
    bool snoop(uint32_t address, char op, int &penaltyCycles);
    int getBlockBits() const { return b; }
};
\end{lstlisting}

\subsection{Bus.hpp}
\begin{lstlisting}[language=C++]
#pragma once
#include <vector>
#include <cstdint>

class Cache;

class Bus {
public:
    std::vector<Cache *> caches;
    int invalidations;
    int dataTrafficBytes;
    int transactions;
    bool broadcast(uint32_t address, char op, int sourceId);
    void registerCache(Cache *cache);
    int bus_cycles = -1;
    Bus();
};
\end{lstlisting}

\subsection{Core.hpp}
\begin{lstlisting}[language=C++]
#pragma once
#include <fstream>
#include <sstream>
#include <string>

class Bus;
class Cache;

using namespace std;

class Core {
public:
    ifstream infile;
    int id;
    Cache* cache;
    int totalAccesses;
    int readCount, writeCount;
    int cacheMisses;
    int evictions;
    int writebacks;
    int totalCycles;
    int idleCycles;
    int execCycle;
    int invalidations;
    int dataTraffic;

    bool repeat = false;
    char repeat_op;
    uint32_t repeat_address;

    Core(int id, Cache* cache);
    void recordTrace(const std::string& filename);
    void processTrace(int currentCycle);
};
\end{lstlisting}

\section{False Sharing Analysis}

\subsection{Test Case Description}
To study the impact of false sharing, we created two test cases where multiple processor cores write to adjacent memory locations that map to the same cache line. Each core writes to a distinct memory address, but due to the proximity of these addresses, they share the same cache line in the L1 cache.

\subsubsection{Test Case 1}
The memory access patterns for the four cores are as follows:
\begin{itemize}
    \item \textbf{Core 0}: Writes to addresses \texttt{0x00000000}, \texttt{0x00010010}, \texttt{0x00020020}, \texttt{0x00030030}.
    \item \textbf{Core 1}: Writes to addresses \texttt{0x00000010}, \texttt{0x00010020}, \texttt{0x00020030}, \texttt{0x00030040}.
    \item \textbf{Core 2}: Writes to addresses \texttt{0x00000020}, \texttt{0x00010030}, \texttt{0x00020040}, \texttt{0x00030050}.
    \item \textbf{Core 3}: Writes to addresses \texttt{0x00000030}, \texttt{0x00010040}, \texttt{0x00020050}, \texttt{0x00030060}.
\end{itemize}

\subsubsection{Test Case 2}
The memory access patterns for the four cores are as follows:
\begin{itemize}
    \item \textbf{Core 0}: Writes to addresses \texttt{0x00000000}, \texttt{0x00100100}, \texttt{0x00200200}, \texttt{0x00300300}.
    \item \textbf{Core 1}: Writes to addresses \texttt{0x00000100}, \texttt{0x00100200}, \texttt{0x00200300}, \texttt{0x00300400}.
    \item \textbf{Core 2}: Writes to addresses \texttt{0x00000200}, \texttt{0x00100300}, \texttt{0x00200400}, \texttt{0x00300500}.
    \item \textbf{Core 3}: Writes to addresses \texttt{0x00000300}, \texttt{0x00100400}, \texttt{0x00200500}, \texttt{0x00300600}.
\end{itemize}

\subsection{Results}
The graph in Figure~\ref{fig:false_sharing} shows the impact of block size on the maximum execution time for Test Case 1. Similarly, Figure~\ref{fig:false_sharing_case2} shows the results for Test Case 2. The results indicate that smaller block sizes lead to reduced execution time, while larger block sizes significantly increase execution time.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{false_sharing_plot.png}
    \caption{Impact of Block Size on Maximum Execution Time (Test Case 1)}
    \label{fig:false_sharing}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{false_sharing_plot2.png}
    \caption{Impact of Block Size on Maximum Execution Time (Test Case 2)}
    \label{fig:false_sharing_case2}
\end{figure}

\subsection{Analysis}
False sharing occurs when multiple cores write to different variables that reside on the same cache line. In these test cases:
\begin{itemize}
    \item Each core writes to a distinct memory address, but due to the proximity of these addresses, they share the same cache line.
    \item When one core writes to its address, the cache line is invalidated in the other cores' caches, forcing them to reload the line from memory or another cache.
    \item This frequent invalidation and reloading of cache lines lead to significant performance degradation, as observed in the increased execution time for larger block sizes.
\end{itemize}

\subsection{Key Observations}
\begin{itemize}
    \item \textbf{Smaller Block Sizes}: Reducing the block size minimizes the likelihood of false sharing, as fewer unrelated variables are packed into the same cache line.
    \item \textbf{Larger Block Sizes}: Increasing the block size exacerbates false sharing, as more unrelated variables are packed into the same cache line, leading to frequent invalidations.
\end{itemize}

\subsection{Conclusion}
False sharing is a critical performance issue in multi-core systems, especially when cores access adjacent memory locations. Optimizing block size and aligning data structures to cache line boundaries can mitigate this issue. These test cases highlight the importance of understanding memory access patterns and their interaction with cache architecture.

\end{document}
